# Frontend Environment Variables - Example Template
# Copy to .env.development.local (dev) or .env.production.local (prod)
# Never commit actual .env.development.local or .env.production.local files

# API Configuration
# Base URL for backend API endpoints
VITE_API_URL=http://localhost:3000

# Ollama Configuration (Local LLM Integration)
# URL for Ollama API
VITE_OLLAMA_API_URL=http://localhost:11434/api/generate
# Model to use (must be installed in Ollama)
VITE_OLLAMA_MODEL=llama3-chatqa:latest
# Language for Ollama responses
VITE_OLLAMA_LANGUAGE=english

# LLM Provider Configuration (via backend proxy)
# Set to true to use Google Gemini instead of local Ollama
# Note: API key is stored securely on backend, not in frontend
VITE_USE_GOOGLE_GEMINI=false
# URL for the LLM proxy endpoint on backend
VITE_LLM_PROXY_URL=http://localhost:3000/api/llm/generate

# Application Settings
# Application display name
VITE_APP_NAME="Path AI"
# Logging level: debug, info, warn, error
VITE_LOG_LEVEL=debug

# Feature Flags
# Enable debug mode and verbose logging
VITE_ENABLE_DEBUG_MODE=true
# Use mock API responses (for development/testing without backend)
VITE_ENABLE_MOCK_API=false

# Video Player Assets (removed)
